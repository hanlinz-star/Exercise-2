{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUC0sciaujUylJuzldAD15",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanlinz-star/Exercise-2/blob/main/Exercise05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "mUI20b0udWQc",
        "outputId": "b8f965d6-095b-4a25-d94d-10ce70188334"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0cea8c8f-78da-45b3-96df-8b3875732e02\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0cea8c8f-78da-45b3-96df-8b3875732e02\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2535520808.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPklfwmab3IV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_df = pd.read_csv(\"dataset_exercise_5_clustering_highway_traffic.csv\", sep=\";\")\n",
        "data_eval__df = pd.read_csv(\"evaluation_dataset_exercise_5_clustering_highway_traffic.csv\", sep=\";\")\n",
        "\n",
        "data_df\n",
        "data_eval_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the DataFrame 'data_df' by columns \"Date\" and \"Interval_5\"\n",
        "data_df.sort_values([\"Date\", \"Interval_5\"])\n",
        "\n",
        "# Extract unique dates from the sorted DataFrame\n",
        "days = np.unique(data_df[['Date']].values.ravel())\n",
        "# Calculate the total number of unique days\n",
        "ndays = len(days)\n",
        "\n",
        "# Group the DataFrame 'data_df' by the \"Date\" column\n",
        "day_subsets_df = data_df.groupby([\"Date\"])\n",
        "\n",
        "# Define the total number of 5-minute intervals in a day\n",
        "nintvals = 288\n",
        "\n",
        "# Create a matrix 'vectorized_day_dataset' filled with NaN values\n",
        "vectorized_day_dataset = np.zeros((ndays, nintvals))\n",
        "vectorized_day_dataset.fill(np.nan)\n",
        "\n",
        "# Loop through each unique day\n",
        "for i in range(0, ndays):\n",
        "    # Get the DataFrame corresponding to the current day\n",
        "    df_t = day_subsets_df.get_group(days[i])\n",
        "\n",
        "    # Loop through each row in the current day's DataFrame\n",
        "    for j in range(len(df_t)):\n",
        "        # Get the current day's DataFrame\n",
        "        df_t = day_subsets_df.get_group(days[i])\n",
        "\n",
        "        # Extract the \"Interval_5\" and \"flow\" values and populate 'vectorized_day_dataset'\n",
        "        vectorized_day_dataset[i, df_t.iloc[j][\"Interval_5\"]] = df_t.iloc[j][\"flow\"]\n",
        "\n",
        "# Print the resulting 'vectorized_day_dataset'\n",
        "print(vectorized_day_dataset)"
      ],
      "metadata": {
        "id": "DLE44PfOeblc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('number of nans',np.sum(np.isnan(vectorized_day_dataset)))\n",
        "print('rate of nans',np.sum(np.isnan(vectorized_day_dataset))/(ndays*nintvals))"
      ],
      "metadata": {
        "id": "yERIcfcehBmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nans_per_time = np.sum(np.isnan(vectorized_day_dataset),0)\n",
        "print(nans_per_time.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "# Create an array 'x_axis' representing the 5-minute intervals\n",
        "x_axis = np.arange(0, nintvals, 1, dtype=int)\n",
        "# Initialize an empty list 'x_axis_hours' to store time values in hours\n",
        "x_axis_hours = []\n",
        "# Convert interval indices to hours and append them to 'x_axis_hours'\n",
        "for i in range(0, len(x_axis)):\n",
        "  x_axis_hours.append(float(x_axis[i]*5)/60)\n",
        "ax.bar(x_axis_hours,height=nans_per_time)\n",
        "\n",
        "\n",
        "ax.set_ylabel('number of missing values')\n",
        "ax.set_xlabel('5-minute intervals')\n",
        "ax.set_title('Time profile of missing values')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FNTjw0hkh7F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nans_per_day = np.sum(np.isnan(vectorized_day_dataset),1)\n",
        "print('number of days with missing value',np.size(np.where(nans_per_day > 0),1))"
      ],
      "metadata": {
        "id": "eWSNuAw0ijTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new figure and axis object using subplots\n",
        "fig, ax = plt.subplots()# a convenient way to create a new figure and a set of subplots.\n",
        "ax.plot(np.array([x_axis_hours,]*ndays).transpose(),vectorized_day_dataset.transpose(),color='#444444',alpha=0.05)\n",
        "# Above line plots the dataset with specified color and transparency.\n",
        "ax.plot(x_axis_hours,np.transpose(np.nanmean(vectorized_day_dataset,0)),color='black')\n",
        "# Above line plots the average of the dataset in black color.\n",
        "\n",
        "ax.set_ylabel('Average flow')\n",
        "ax.set_xlabel('5-minute intervals')\n",
        "plt.xlim(0,24)\n",
        "ax.set_title('Daily profile of flow dynamic')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WcviP90Yiu0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new figure and axis object using subplots\n",
        "fig, ax = plt.subplots()  # This line is a convenient way to create a new figure and a set of subplots.\n",
        "\n",
        "# Create a boxplot for the dataset\n",
        "boxplot = ax.boxplot(vectorized_day_dataset.T, patch_artist=True)\n",
        "\n",
        "# Customize the boxplot appearance\n",
        "for patch in boxplot['boxes']:\n",
        "    patch.set_facecolor('#444444')  # Set the box color to gray\n",
        "for median in boxplot['medians']:\n",
        "    median.set(color='black', linewidth=2)  # Set median line color to black\n",
        "\n",
        "# Set the y-axis label\n",
        "ax.set_ylabel('Flow')\n",
        "\n",
        "# Set the x-axis label\n",
        "ax.set_xlabel('5-minute intervals')\n",
        "\n",
        "# Set the x-axis limits to be between 0 and 24\n",
        "plt.xlim(0, 24)\n",
        "\n",
        "# Set the title of the plot\n",
        "ax.set_title('Daily Profile of Flow Dynamics (Boxplot)')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WpUrJcQGkde-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "# Create an array 'day_of_week' to store the day of the week for each unique date\n",
        "day_of_week = np.zeros((ndays))\n",
        "\n",
        "# Loop through each unique date\n",
        "for i in range(0, ndays):\n",
        "    # Parse the current date from a string to a datetime object\n",
        "    day_dt = datetime.datetime.strptime(str(days[i]), '%Y%m%d')\n",
        "\n",
        "    # Get the day of the week (1 for Monday, 2 for Tuesday, ..., 7 for Sunday)\n",
        "    day_of_week[i] = day_dt.isoweekday()"
      ],
      "metadata": {
        "id": "H3vXOkcJm-9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new figure and axis object using subplots\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Iterate through each day of the week (from 1 to 7)\n",
        "for i in range(1, 8):\n",
        "    # Find the indices of days that correspond to the current day of the week\n",
        "    day_of_week_index_t = np.where(day_of_week == i)\n",
        "\n",
        "    # Calculate the number of days that match the current day of the week\n",
        "    ndays_t = np.size(day_of_week_index_t[0])\n",
        "\n",
        "    # Plot the average flow for the current day of the week\n",
        "    ax.plot(x_axis_hours,\n",
        "            np.nanmean(vectorized_day_dataset[day_of_week_index_t[0], :].transpose(), 1),\n",
        "            label='day-of-week ' + str(i))\n",
        "    # This line plots the average flow for the current day of the week.\n",
        "    # 'np.nanmean()' calculates the mean while handling NaN values.\n",
        "\n",
        "# Set the y-axis label\n",
        "ax.set_ylabel('Average flow')\n",
        "\n",
        "# Set the x-axis label\n",
        "ax.set_xlabel('5-minute intervals')\n",
        "\n",
        "# Set the x-axis limits to be between 0 and 24\n",
        "plt.xlim(0, 24)\n",
        "\n",
        "# Set the title of the plot\n",
        "ax.set_title('Daily Profile of Flow Dynamics')\n",
        "\n",
        "# Add a legend indicating the day of the week\n",
        "ax.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HGTDhb6mnVMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "#clusters = KMeans(n_clusters=10, random_state=0, n_init=\"auto\").fit(vectorized_day_dataset)"
      ],
      "metadata": {
        "id": "qicl5afQoEhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 10\n",
        "clusters = None\n",
        "#print(np.where(nans_per_day > 0)[0])\n",
        "vectorized_day_dataset_no_nans = vectorized_day_dataset[np.where(nans_per_day == 0)[0],:]\n",
        "days_not_nans = days[np.where(nans_per_day == 0)[0]]\n",
        "\n",
        "# BELOW lines enables you to comment in and out clustering method you want to use note that GMM have different ouput and thus labels are extracted differently\n",
        "clusters = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
        "# clusters = AgglomerativeClustering(n_clusters=n_clusters,metric='euclidean', linkage='ward').fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
        "# clusters = DBSCAN(eps=500, min_samples = 2).fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
        "\n",
        "if clusters is not None:\n",
        "  cluster_labels = clusters.labels_\n",
        "\n",
        "#cluster_labels = GaussianMixture(n_components=n_clusters).fit(vectorized_day_dataset_no_nans).predict(vectorized_day_dataset_no_nans) #check the parameters at  https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_init.html#sphx-glr-auto-examples-mixture-plot-gmm-init-py\n",
        "\n",
        "\n",
        "print(cluster_labels)"
      ],
      "metadata": {
        "id": "wuiB3GIroG0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from matplotlib.patches import Polygon\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib import gridspec\n",
        "from matplotlib.patches import Patch\n",
        "from matplotlib import colors\n",
        "\n",
        "def assign_colors(n_clusters, days, assigments):\n",
        "\n",
        "    days_colors = []\n",
        "    color_to_cluster = []\n",
        "    style_to_cluster = []\n",
        "    weekend_colors = ['#67001f','#d6604d','#fdae61','#f46d43','#d53e4f','#9e0142','#f768a1','#f1c232']#,'#fe9929','#cc4c02','#e31a1c','#737373','#bdbdbd','#252525','#bcbddc']\n",
        "#    weekend_school_colors = ['#c2a5cf','#f1b6da','#8e0152','#c51b7d','#de77ae','#ae017e','#fcc5c0','#e31a1c','#737373','#bdbdbd']\n",
        "#    bank_holidays_colors = ['#543005','#dfc27d','#bf812d','#8c510a']\n",
        "    mixed_colors = ['#4d4d4d','#35978f','#bababa','#878787']\n",
        "    weekday_colors = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#cab2d6','#6a3d9a','#b15928','#8dd3c7','#bebada','#fb8072','#b3de69','#bc80bd','#fccde5','#ccebc5','#35978f','#80cdc1']\n",
        "\n",
        "    cluster_id_weekdays_share = []\n",
        "    cluster_id_weekend_share = []\n",
        "    cluster_id_all_days = []\n",
        "\n",
        "    for i in range(0,n_clusters):\n",
        "        color_to_cluster.append(None)\n",
        "        style_to_cluster.append(None)\n",
        "        cluster_id_weekdays_share.append(0)\n",
        "        cluster_id_weekend_share.append(0)\n",
        "        cluster_id_all_days.append(0)\n",
        "\n",
        "    for i in range(0,len(days)):\n",
        "        #print(i,assigments[i],len(assigments),len(cluster_id_all_days))\n",
        "        if assigments[i] is not None:\n",
        "            cluster_id_all_days[assigments[i]] += 1\n",
        "            if '-' in str(days[i]):\n",
        "                pomT = datetime.datetime.strptime(str(days[i]),'%Y-%m-%d')\n",
        "            else:\n",
        "                pomT = datetime.datetime.strptime(str(days[i]),'%Y%m%d')\n",
        "\n",
        "            if int(pomT.weekday()) < 5:\n",
        "                cluster_id_weekdays_share[assigments[i]] += 1\n",
        "            else:\n",
        "                cluster_id_weekend_share[assigments[i]] += 1\n",
        "\n",
        "    print('cluster_id_weekdays_share',cluster_id_weekdays_share)\n",
        "    print('cluster_id_weekend_share',cluster_id_weekend_share)\n",
        "    for i in range(0,len(days)):\n",
        "        if assigments[i] is not None:\n",
        "            cluster_idx = assigments[i]\n",
        "            if '-' in str(days[i]):\n",
        "                pomT = datetime.datetime.strptime(str(days[i]),'%Y-%m-%d')\n",
        "            else:\n",
        "                pomT = datetime.datetime.strptime(str(days[i]),'%Y%m%d')\n",
        "            if color_to_cluster[assigments[i]] is None:\n",
        "                if cluster_id_weekend_share[cluster_idx] / float(cluster_id_all_days[cluster_idx]) > 0.6:\n",
        "                        color_to_cluster[assigments[i]] = weekend_colors.pop()\n",
        "                        style_to_cluster[assigments[i]] = ':'\n",
        "                elif cluster_id_weekdays_share[cluster_idx] / float(cluster_id_all_days[cluster_idx]) > 0.6:\n",
        "                        color_to_cluster[assigments[i]] = weekday_colors.pop(0)\n",
        "                        style_to_cluster[assigments[i]] = '-'\n",
        "                else:\n",
        "                    color_to_cluster[assigments[i]] = mixed_colors.pop()\n",
        "                    style_to_cluster[assigments[i]] = ':'\n",
        "\n",
        "            days_colors.append(color_to_cluster[assigments[i]])\n",
        "        else:\n",
        "            days_colors.append(None)\n",
        "\n",
        "    return days_colors,color_to_cluster,style_to_cluster\n",
        "\n",
        "\n",
        "def calmap(ax, year, data, days, assigments, n_clusters,days_colors,color_to_cluster,\n",
        "           limit_graphics=False):\n",
        "\n",
        "    ax.tick_params('x', length=0, labelsize=\"medium\", which='major')\n",
        "    ax.tick_params('y', length=0, labelsize=\"x-small\", which='major')\n",
        "\n",
        "    # Month borders\n",
        "\n",
        "    xticks, labels = [], []\n",
        "    start = datetime.datetime(year,1,1).weekday()\n",
        "\n",
        "    for month in range(1,13):\n",
        "\n",
        "        first = datetime.datetime(year, month, 1)\n",
        "        last = first + relativedelta(months=1, days=-1)\n",
        "\n",
        "        y0 = first.weekday()\n",
        "        y1 = last.weekday()\n",
        "        x0 = (int(first.strftime(\"%j\"))+start-1)//7\n",
        "        x1 = (int(last.strftime(\"%j\"))+start-1)//7\n",
        "\n",
        "        P = [ (x0,   y0), (x0,    7),  (x1,   7),\n",
        "              (x1,   y1+1), (x1+1,  y1+1), (x1+1, 0),\n",
        "              (x0+1,  0), (x0+1,  y0) ]\n",
        "\n",
        "        xticks.append(x0 +(x1-x0+1)/2)\n",
        "        labels.append(first.strftime(\"%b\"))\n",
        "        poly = Polygon(P, edgecolor=\"black\", facecolor=\"None\",\n",
        "\n",
        "                       linewidth=1, zorder=20, clip_on=False)\n",
        "\n",
        "        ax.add_artist(poly)\n",
        "\n",
        "    line = Line2D([0,53],[5,5],linewidth=1, zorder = 20,color=\"black\",linestyle='dashed')\n",
        "    ax.add_artist(line)\n",
        "\n",
        "    if not limit_graphics:\n",
        "        ax.set_xticks(xticks)\n",
        "        ax.set_xticklabels(labels)\n",
        "        ax.set_yticks(0.5 + np.arange(7))\n",
        "        ax.set_yticklabels([\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n",
        "        ax.set_title(\"{}\".format(year), weight=\"semibold\")\n",
        "    else:\n",
        "        plt.tick_params(\n",
        "            axis='x',          # changes apply to the x-axis\n",
        "            which='both',      # both major and minor ticks are affected\n",
        "            bottom=False,      # ticks along the bottom edge are off\n",
        "            top=False,         # ticks along the top edge are off\n",
        "            labelbottom=False)\n",
        "        plt.tick_params(\n",
        "            axis='y',          # changes apply to the x-axis\n",
        "            which='both',      # both major and minor ticks are affected\n",
        "            left=False,      # ticks along the bottom edge are off\n",
        "            right=False,         # ticks along the top edge are off\n",
        "            labelleft=False)\n",
        "\n",
        "    # Clearing first and last day from the data\n",
        "    valid = datetime.datetime(year, 1, 1).weekday()\n",
        "    data[:valid,0] = np.nan\n",
        "    valid = datetime.datetime(year, 12, 31).weekday()\n",
        "    # data[:,x1+1:] = np.nan\n",
        "    data[valid+1:,x1] = np.nan\n",
        "\n",
        "    for i in range(0,len(days)):\n",
        "        if '-' in str(days[i]):\n",
        "            pomT = datetime.datetime.strptime(str(days[i]),'%Y-%m-%d')\n",
        "        else:\n",
        "            pomT = datetime.datetime.strptime(str(days[i]),'%Y%m%d')\n",
        "        week_number = int(pomT.strftime(\"%W\"))\n",
        "        day_of_week = int(pomT.weekday())\n",
        "        data[day_of_week,week_number] = assigments[i]\n",
        "\n",
        "\n",
        "    act_date = datetime.datetime(year,1,1)\n",
        "    while (act_date.year == year):\n",
        "\n",
        "        week_number = int(act_date.strftime(\"%W\"))\n",
        "        day_of_week = int(act_date.weekday())\n",
        "        doy_id = act_date.timetuple().tm_yday\n",
        "        if doy_id<5 and week_number > 53:\n",
        "            week_number = 0\n",
        "\n",
        "        act_date = act_date + datetime.timedelta(days=1)\n",
        "\n",
        "    #pomT = datetime.datetime.strptime('2017-01-01','%Y-%m-%d')\n",
        "    #week_number = int(pomT.strftime(\"%V\"))\n",
        "    #day_of_week = int(pomT.weekday())\n",
        "    #print(week_number,day_of_week)\n",
        "    #doy_id = pomT.timetuple().tm_yday\n",
        "    #if doy_id<5 and week_number > 0:\n",
        "    #    week_number = 0\n",
        "    #data[day_of_week,week_number] = len(clusters)+10\n",
        "\n",
        "    # Showing data\n",
        "    cmap = plt.cm.spring  # Can be any colormap that you want after the cm\n",
        "    cmap.set_bad(color='white')\n",
        "\n",
        "    #ax.imshow(data, extent=[0,53,0,7], zorder=10, vmin=0, vmax=len(clusters)+10,\n",
        "    #          cmap=cmap, origin=\"lower\", alpha=.75)\n",
        "\n",
        "    cmap = colors.ListedColormap(color_to_cluster)\n",
        "    bounds=[-0.1]\n",
        "    step = 1\n",
        "    for i in range(0,n_clusters):\n",
        "        bounds.append(i-0.1+step)\n",
        "    norm = colors.BoundaryNorm(bounds, cmap.N)\n",
        "    #print(color_to_cluster)\n",
        "   #print(bounds)\n",
        "    #print(norm)\n",
        "\n",
        "    #print(color_to_cluster)\n",
        "    #print(bounds)\n",
        "    #print(cmap)\n",
        "    #exit(0)\n",
        "\n",
        "    ax.imshow(data, extent=[0,53,0,7], zorder=10, interpolation='nearest', origin='lower',cmap=cmap, norm=norm)\n",
        "\n",
        "def make_calendar_visualization_figure(days,assigments,n_clusters,years,days_colors,color_to_cluster,\n",
        "                                       save_figure: str = None, show_figure:bool = True, limit_graphics = False):\n",
        "\n",
        "    fig = plt.figure(figsize=(8,1.5*len(years)), dpi=100)\n",
        "    X = np.linspace(-1,1, 53*7)\n",
        "\n",
        "    for i, obj in enumerate(years):\n",
        "\n",
        "        pom_s = str(len(years))+'1'+str(i+1)\n",
        "        print(pom_s)\n",
        "\n",
        "        ax = plt.subplot(int(pom_s), xlim=[0, 53], ylim=[0, 7], frameon=False, aspect=1)\n",
        "        I = 1.2 - np.cos(X.ravel()) + np.random.normal(0,.2, X.size)\n",
        "        I = I.reshape(53,7).T\n",
        "        I.fill(np.nan)\n",
        "        calmap(ax, int(obj), I.reshape(53,7).T, days, assigments, n_clusters,days_colors,color_to_cluster, limit_graphics)\n",
        "\n",
        "    #   ax = plt.subplot(212, xlim=[0,53], ylim=[0,7], frameon=False, aspect=1)\n",
        "    #  I = 1.1 - np.cos(X.ravel()) + np.random.normal(0,.2, X.size)\n",
        "    #   calmap(ax, 2018, I.reshape(53,7).T)\n",
        "\n",
        "    #ax = plt.subplot(313, xlim=[0,53], ylim=[0,7], frameon=False, aspect=1)\n",
        "    #I = 1.0 - np.cos(X.ravel()) + np.random.normal(0,.2, X.size)\n",
        "    #calmap(ax, 2019, I.reshape(53,7).T)\n",
        "    if save_figure:\n",
        "        plt.savefig(save_figure)\n",
        "\n",
        "    if show_figure or save_figure is None:\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def make_figure_centroids(x,y,color_to_cluster,style_to_cluster,cluster_ids,minY = None,maxY = None,\n",
        "                          save_figure: str = None, show_figure:bool = True):\n",
        "\n",
        "    #print(color_to_cluster)\n",
        "    fig = plt.figure(figsize=(8,3))\n",
        "    ax = fig.add_subplot(111)\n",
        "    for i in range(0,len(x)):\n",
        "        #print(i,color_to_cluster[i],style_to_cluster[i])\n",
        "        #print(y[i])\n",
        "        ax.plot(x[i],y[i],style_to_cluster[i], color=color_to_cluster[i], label=str(cluster_ids[i]))\n",
        "    ax.set_xlabel('Time of day')\n",
        "    ax.set_ylabel('Flow')\n",
        "    if minY is not None and maxY is not None:\n",
        "        ax.set_ylim([minY, maxY])\n",
        "    plt.legend()\n",
        "\n",
        "    if save_figure:\n",
        "        plt.savefig(save_figure)\n",
        "\n",
        "    if show_figure or save_figure is None:\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "n4LE_7S6oKbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of clusters by finding unique values in 'cluster_labels'\n",
        "n_clusters_t = len(np.unique(cluster_labels))\n",
        "\n",
        "# Assign colors to days based on clusters\n",
        "days_colors, color_to_cluster, style_to_cluster = assign_colors(n_clusters_t, days_not_nans, cluster_labels)\n",
        "# The function 'assign_colors' is used to determine colors and styles for visualization.\n",
        "\n",
        "# Create a calendar visualization figure\n",
        "make_calendar_visualization_figure(days_not_nans, cluster_labels, n_clusters_t, [2021], days_colors,\n",
        "                                   color_to_cluster, save_figure=None)\n",
        "# This function 'make_calendar_visualization_figure' is used to generate a visualization based on the provided data and parameters.\n",
        "# 'days_not_nans' are the days, 'cluster_labels' are the cluster labels, 'n_clusters_t' is the number of clusters,\n",
        "# '[2021]' represents the year, 'days_colors' represent the assigned colors for each day, 'color_to_cluster' maps colors to clusters,\n",
        "# and 'save_figure' is an optional parameter to save the generated figure (can be None if not saving)."
      ],
      "metadata": {
        "id": "9cbrLtK-opdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty lists to store centroid data\n",
        "centroids_xx = []  # x-axis values for centroids\n",
        "centroids_yy_daytypes = []  # y-axis values for centroids, grouped by day types\n",
        "cluster_ids = []  # Cluster IDs\n",
        "\n",
        "# Iterate through each cluster\n",
        "for i in range(0, n_clusters_t):\n",
        "    # Store the x-axis values for centroids (hours of the day)\n",
        "    centroids_xx.append(x_axis_hours)\n",
        "\n",
        "    # Calculate the y-axis values for centroids (average flow for each 5-minute interval)\n",
        "    centroid_yy = list(np.nanmean(vectorized_day_dataset_no_nans[np.where(cluster_labels == i)[0], :], 0).transpose())\n",
        "    centroids_yy_daytypes.append(centroid_yy)\n",
        "\n",
        "    # Store the cluster ID\n",
        "    cluster_ids.append(i)\n",
        "\n",
        "# Generate a figure displaying the centroids\n",
        "make_figure_centroids(centroids_xx, centroids_yy_daytypes, color_to_cluster, style_to_cluster, cluster_ids)\n",
        "# The function 'make_figure_centroids' is used to create a visualization of the centroids,\n",
        "# with the provided data and parameters."
      ],
      "metadata": {
        "id": "ZizZG4vio3UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the evaluation DataFrame by columns \"Date\" and \"Interval_5\"\n",
        "data_eval_df.sort_values([\"Date\", \"Interval_5\"])\n",
        "\n",
        "# Extract unique dates from the sorted evaluation DataFrame\n",
        "days_eval = np.unique(data_eval_df[['Date']].values.ravel())\n",
        "# Calculate the total number of unique days in the evaluation dataset\n",
        "ndays_eval = len(days_eval)\n",
        "\n",
        "# Group the evaluation DataFrame by the \"Date\" column\n",
        "day_eval_subsets_df = data_eval_df.groupby([\"Date\"])\n",
        "\n",
        "# Initialize a matrix 'vectorized_day_dataset_eval' filled with NaN values\n",
        "vectorized_day_dataset_eval = np.zeros((ndays_eval, nintvals))\n",
        "vectorized_day_dataset_eval.fill(np.nan)\n",
        "# This section initializes a 2D array to store the evaluation dataset and fills it with NaN values.\n",
        "\n",
        "# Loop through each unique day in the evaluation dataset\n",
        "for i in range(0, ndays_eval):\n",
        "    # Get the DataFrame corresponding to the current day\n",
        "    df_t = day_eval_subsets_df.get_group(days_eval[i])\n",
        "\n",
        "    # Loop through each row in the current day's DataFrame\n",
        "    for j in range(len(df_t)):\n",
        "        # Get the current day's DataFrame (this line is redundant)\n",
        "        df_t = day_eval_subsets_df.get_group(days_eval[i])\n",
        "\n",
        "        # Extract the \"Interval_5\" and \"flow\" values and populate 'vectorized_day_dataset_eval'\n",
        "        vectorized_day_dataset_eval[i, df_t.iloc[j][\"Interval_5\"]] = df_t.iloc[j][\"flow\"]\n",
        "\n",
        "# Print the resulting 'vectorized_day_dataset_eval'\n",
        "print(vectorized_day_dataset_eval)"
      ],
      "metadata": {
        "id": "Dy8kqg5Qo7oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of NaN values in the evaluation dataset\n",
        "print('Number of NaNs:', np.sum(np.isnan(vectorized_day_dataset_eval)))\n",
        "\n",
        "# Calculate the rate of NaN values in the evaluation dataset\n",
        "print('Rate of NaNs:', np.sum(np.isnan(vectorized_day_dataset_eval)) / (ndays_eval * nintvals))\n",
        "\n",
        "# Calculate the number of days with missing values\n",
        "nans_per_day_eval = np.sum(np.isnan(vectorized_day_dataset_eval), 1)\n",
        "print('Number of days with missing values:', np.size(np.where(nans_per_day_eval > 0)))\n",
        "\n",
        "# Filter out days with no missing values and create a new dataset\n",
        "vectorized_day_dataset_no_nans_eval = vectorized_day_dataset_eval[np.where(nans_per_day_eval == 0)[0], :]\n",
        "days_not_nans_eval = days_eval[np.where(nans_per_day_eval == 0)[0]]\n",
        "\n",
        "# Calculate the final number of days in the evaluation dataset after removing missing values\n",
        "print('Final number of days in evaluation dataset:', len(days_not_nans_eval))\n",
        "\n",
        "# Print the list of days in the evaluation dataset with no missing values\n",
        "print('List of days without missing values:', days_not_nans_eval)\n",
        "\n",
        "# Calculate the total number of days in the filtered evaluation dataset\n",
        "ndays_eval_not_nans = len(days_not_nans_eval)"
      ],
      "metadata": {
        "id": "1cklC8PQrtHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the pairwise_distances function from scikit-learn's metrics library\n",
        "import sklearn.metrics.pairwise as dis_lib\n",
        "\n",
        "# Define a function to find the closest centroid to a new data point within a specified day-time interval range\n",
        "def find_the_closest_centroid(centroids, new_day, from_interval: int, to_interval: int):\n",
        "    closest_centroid = None\n",
        "    closest_dist = None\n",
        "\n",
        "    # Iterate through each centroid\n",
        "    for i in range(0, len(centroids)):\n",
        "        # Calculate the Euclidean distance between the centroid and the new data point\n",
        "        ed_t = dis_lib.paired_distances(centroids[i], new_day, metric='euclidean')\n",
        "\n",
        "        # Check if the current centroid is closer than the previously closest one\n",
        "        if closest_centroid is None or closest_dist > ed_t:\n",
        "            closest_centroid = i\n",
        "            closest_dist = ed_t\n",
        "\n",
        "    return closest_centroid\n",
        "\n",
        "# Initialize a list to store centroid data\n",
        "centroids = []\n",
        "\n",
        "# Calculate centroids for each cluster\n",
        "for i in np.unique(cluster_labels):\n",
        "    centroid = np.nanmean(vectorized_day_dataset_no_nans[np.where(cluster_labels == i)[0], :], 0).reshape(1, nintvals)\n",
        "    centroids.append(centroid)\n",
        "\n",
        "# Define the number of past intervals to consider for classification\n",
        "n_past_intervals_for_classification = 5\n",
        "\n",
        "# Initialize variables to calculate accuracy metrics\n",
        "total_mae = 0\n",
        "total_mape = 0\n",
        "prediction_counts = 0\n",
        "\n",
        "# Loop through each day in the evaluation dataset with no missing values\n",
        "for i in range(0, ndays_eval_not_nans):\n",
        "    # Loop through intervals from n_past_intervals_for_classification to nintvals - 1\n",
        "    for j in range(n_past_intervals_for_classification, nintvals - 1):\n",
        "        # Find the closest centroid for the current data point\n",
        "        centroid_index = find_the_closest_centroid(centroids, vectorized_day_dataset_no_nans_eval[i].reshape(1, nintvals), j - n_past_intervals_for_classification, j)\n",
        "\n",
        "        # Predict the value for the next interval\n",
        "        predicted_value = centroids[centroid_index][0, j + 1]\n",
        "\n",
        "        # Calculate Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE)\n",
        "        mae_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
        "        mape_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1]) / float(vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
        "\n",
        "        # Accumulate MAE, MAPE, and count of predictions\n",
        "        total_mae += mae_t\n",
        "        total_mape += mape_t\n",
        "        prediction_counts += 1\n",
        "\n",
        "# Calculate and print the prediction accuracy metrics\n",
        "print('Prediction accuracy MAE:', total_mae / prediction_counts)\n",
        "print('Prediction accuracy MAPE:', total_mape / prediction_counts)\n"
      ],
      "metadata": {
        "id": "tRJ6bxg-sN7j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}